{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of v3 - 6 - lesson7-human-numbers.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9ELqDZcQ22Ty","colab_type":"text"},"source":["# Human numbers\n","\n","We're now going to jump into human numbers which is lesson7-human-numbers.ipynb. This is a dataset that I created which literally just contains all the numbers from 1 to 9,999 written out in English.\n","\n","We're going to try to create a language model that can predict the next word in this document. It's just a toy example for this purpose. In this case, we only have one document. That one document is the list of numbers. So we can use a TextList to create an item list with text in for the training of the validation."]},{"cell_type":"markdown","metadata":{"id":"gCGWB9x925aj","colab_type":"text"},"source":["## Lesson 7 Notes\n","\n","[from hiromis](https://github.com/hiromis/notes/blob/master/Lesson7.md)\n","\n","\n","[discussion thread](https://forums.fast.ai/t/lesson-7-in-class-chat/32554/118)\n","\n","[advanced discussion thread](https://forums.fast.ai/t/lesson-7-further-discussion/32555)"]},{"cell_type":"markdown","metadata":{"id":"cvSuBXYM3HqV","colab_type":"text"},"source":["## Recurrent Neural Network - RNN\n","\n","[video timing](https://www.youtube.com/watch?v=nWpdkZE2_cc&feature=youtu.be&t=5911)\n","\n","see Hiromis notes.  [from hiromis](https://github.com/hiromis/notes/blob/master/Lesson7.md)\n"]},{"cell_type":"code","metadata":{"id":"Wzd2AYja22T1","colab_type":"code","colab":{}},"source":["from fastai.text import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qhy5Dwm722T6","colab_type":"code","colab":{}},"source":["bs=64"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vp5pB8lA22T_","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"9wYQF4Fy22UA","colab_type":"code","outputId":"dbe73b29-d40e-45ff-e7f6-ad6a17a48888","colab":{}},"source":["path = untar_data(URLs.HUMAN_NUMBERS)\n","path.ls()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PosixPath('/home/ubuntu/.fastai/data/human_numbers/train.txt'),\n"," PosixPath('/home/ubuntu/.fastai/data/human_numbers/valid.txt')]"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"mwVnUUfK22UF","colab_type":"code","colab":{}},"source":["def readnums(d): return [', '.join(o.strip() for o in open(path/d).readlines())]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O2Pn0I722UH","colab_type":"code","outputId":"eb29720b-5987-4b30-b47f-ee6649f44d5c","colab":{}},"source":["train_txt = readnums('train.txt'); train_txt[0][:80]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirt'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"YDoluVVJJR1Z","colab_type":"text"},"source":["NOTE: we check the first 80 characters from the training data set."]},{"cell_type":"code","metadata":{"id":"yJaIiedd22UK","colab_type":"code","outputId":"bf67c32e-2562-4963-fc29-2e99dd82a86c","colab":{}},"source":["valid_txt = readnums('valid.txt'); valid_txt[0][-80:]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' nine thousand nine hundred ninety eight, nine thousand nine hundred ninety nine'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"yJovjgEsJH9j","colab_type":"text"},"source":["NOTE: we check the first 80 characters from the validation data set."]},{"cell_type":"code","metadata":{"id":"trhnLfSS22UN","colab_type":"code","colab":{}},"source":["train = TextList(train_txt, path=path)\n","valid = TextList(valid_txt, path=path)\n","\n","src = ItemLists(path=path, train=train, valid=valid).label_for_lm()\n","data = src.databunch(bs=bs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dISVeOb9nZyu","colab_type":"text"},"source":["NOTE:  In this case, the validation set is the numbers from 8,000 onwards, and the training set is 1 to 8,000. We can combine them together, turn that into a data bunch. \n","\n","`label_for_lm` is a special labelling method for language models. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"KeMYpGBs22UR","colab_type":"code","outputId":"ae4fcd96-1a3b-49a9-9158-f259efb9c016","colab":{}},"source":["# display the first 80 characters\n","train[0].text[:80]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'xxbos one , two , three , four , five , six , seven , eight , nine , ten , eleve'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"FSHKGE7c31W3","colab_type":"text"},"source":["NOTE:  We only have one document, so `train[0]` is the document grab its `.text` that's how you grab the contents of a text list, and here are the first 80 characters. It starts with a special token `xxbos`. Anything starting with `xx` is a special fast.ai token, `bos` is the beginning of stream token. It basically says this is the start of a document, and it's very helpful in NLP to know when documents start so that your models can learn to recognize them."]},{"cell_type":"code","metadata":{"id":"yZi1pDhC22UU","colab_type":"code","outputId":"7d805982-7251-407d-a69c-b01a41f95466","colab":{}},"source":["len(data.valid_ds[0][0].data)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13017"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"jTy0U58y38nc","colab_type":"text"},"source":["NOTE:  The validation set contains 13,017 tokens. So 13,017 words or punctuation marks because everything between spaces is a separate token."]},{"cell_type":"code","metadata":{"id":"vT9Ii6D622UX","colab_type":"code","outputId":"ac3f137d-e286-44f9-c0d9-e75fe0e77c09","colab":{}},"source":["# bptt = sequence length (back prop through time)\n","data.bptt, len(data.valid_dl)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70, 3)"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"dyh6sIaO22UZ","colab_type":"code","outputId":"6ed9ff31-ddbb-4d66-c131-343d6453621f","colab":{}},"source":["# total batches (total number of tokens / sequence length / batch size)\n","13017/70/bs"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.905580357142857"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"7b4X6nD04EvC","colab_type":"text"},"source":["NOTE:  The batch size that we asked for was 64, and then by default it uses something called `bptt` of 70. `bptt`, as we briefly mentioned, stands for **\"back prop through time\"**. **That's the sequence length**. \n","\n","For each of our 64 document segments, we split it up into lists of 70 words that we look at at one time. So what we do for the validation set is we grab this entire string of 13,000 tokens, and then we split it into 64 roughly equal sized sections. **They're 64 roughly equally sized segments**. So we take the first 1/64 of the document - piece 1. The second 1/64 - piece 2.\n","\n","Then for each of those 1/64 of the document, we then split those into pieces of length 70. So let's now say for those 13,000 tokens, how many batches are there? Well, divide by batch size and divide by 70, so there's going to be 3 batches."]},{"cell_type":"code","metadata":{"id":"fMHbQPFV22Uc","colab_type":"code","colab":{}},"source":["# iterator for the DataLoader\n","it = iter(data.valid_dl)\n","x1,y1 = next(it)\n","x2,y2 = next(it)\n","x3,y3 = next(it)\n","it.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-bfeI5122Uf","colab_type":"code","outputId":"5e117f21-6df7-418a-e583-4dffbe190850","colab":{}},"source":["# then add the number of elements\n","x1.numel()+x2.numel()+x3.numel()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13440"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"pWBw2PKN4fR6","colab_type":"text"},"source":["NOTE:  Let's grab an iterator for a data loader, grab 1 2 3 batches (the X and the Y), and let's add up the number of elements."]},{"cell_type":"code","metadata":{"id":"q4Svy5UM22Uk","colab_type":"code","outputId":"4ea79e1a-7301-46ab-8f72-223fa336d314","colab":{}},"source":["x1.shape,y1.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 70]), torch.Size([64, 70]))"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"x_-7YTBy22Uq","colab_type":"code","outputId":"fe325f76-485c-49fb-dda6-937c664c7467","colab":{}},"source":["x2.shape,y2.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 70]), torch.Size([64, 70]))"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"zQiR_2_n4nh0","colab_type":"text"},"source":["NOTE:  As you can see, it's 64 by 70. \n","`bppt` add a bit of shuffling."]},{"cell_type":"code","metadata":{"id":"BMyOV-Lq22Uu","colab_type":"code","outputId":"7b07e91e-5742-45cf-d228-305cbe0aaecb","colab":{}},"source":["# first batch of x - numericalized\n","x1[:,0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 2,  8, 10, 11, 12, 10,  9,  8,  9, 13, 18, 24, 18, 14, 15, 10, 18,  8,\n","         9,  8, 18, 24, 18, 10, 18, 10,  9,  8, 18, 19, 10, 25, 19, 22, 19, 19,\n","        23, 19, 10, 13, 10, 10,  8, 13,  8, 19,  9, 19, 34, 16, 10,  9,  8, 16,\n","         8, 19,  9, 19, 10, 19, 10, 19, 19, 19], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"oQ3oTaSM22U5","colab_type":"code","outputId":"082721e4-65c2-4172-b9b7-00ecf9a2d6c6","colab":{}},"source":["# first bach of y - numericalized\n","y1[:,0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 18, 26,  9,  8, 11, 31, 18, 25,  9, 10, 14, 10,  9,  8, 14, 10, 18,\n","        25, 18, 10, 17, 10, 17,  8, 17, 20, 18,  9,  9, 19,  8, 10, 15, 10, 10,\n","        12, 10, 12,  8, 12, 13, 19,  9, 19, 10, 23, 10,  8,  8, 15, 16, 19,  9,\n","        19, 10, 23, 10, 18,  8, 18, 10, 10,  9], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"dW5OFkgP58iY","colab_type":"text"},"source":["NOTE:  So here, you can see the first batch of X (remember, we've numeric aliased all these) and here's the first batch of Y. And you'll see here x1 is ``[2, 18, 10, 11, 8, ...]`, y1 is `[18, 10, 11, 8, ...].` So **y1 is offset by 1 from x1**. Because that's what you want to do with a language model. We want to predict the next word. So after 2 (x1:0), should come 18 (y1:0), and after 8(x1:1), should come 18 (y1:1).\n","\n"]},{"cell_type":"code","metadata":{"id":"UWPKvTn722U-","colab_type":"code","colab":{}},"source":["v = data.valid_ds.vocab"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9FNWpWpYyGCm","colab_type":"text"},"source":["### batch 1"]},{"cell_type":"code","metadata":{"id":"uHmf_5rd22VA","colab_type":"code","outputId":"57cd4f0f-d36c-4c54-ae87-11c86ec8188b","colab":{}},"source":["v.textify(x1[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'xxbos eight thousand one , eight thousand two , eight thousand three , eight thousand four , eight thousand five , eight thousand six , eight thousand seven , eight thousand eight , eight thousand nine , eight thousand ten , eight thousand eleven , eight thousand twelve , eight thousand thirteen , eight thousand fourteen , eight thousand fifteen , eight thousand sixteen , eight thousand seventeen , eight'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"Ff1xLdwnDv1c","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"rlu0EDTg22VD","colab_type":"code","outputId":"d50ed84f-fd96-48c8-9c4e-ba7c5bbd0961","colab":{}},"source":["v.textify(y1[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'eight thousand one , eight thousand two , eight thousand three , eight thousand four , eight thousand five , eight thousand six , eight thousand seven , eight thousand eight , eight thousand nine , eight thousand ten , eight thousand eleven , eight thousand twelve , eight thousand thirteen , eight thousand fourteen , eight thousand fifteen , eight thousand sixteen , eight thousand seventeen , eight thousand'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"_gM3vskP6SIY","colab_type":"text"},"source":["NOTE:  You can grab the vocab for this dataset, and a vocab has a textify so if we look at exactly the same thing but with `textify`, that will just look it up in the vocab. So here you can see `**xxbos** eight thousand one` where else in the y, there's no `xxbos`, it's just `eight thousand one`. So after `xxbos` is `eight`, after `eight` is `thousand`, after `thousand` is `one`."]},{"cell_type":"code","metadata":{"id":"uI9_vaVa22VG","colab_type":"code","outputId":"8f4ee21d-cfda-438b-aff5-f397a1bb6dce","colab":{}},"source":["v.textify(x2[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'thousand eighteen , eight thousand nineteen , eight thousand twenty , eight thousand twenty one , eight thousand twenty two , eight thousand twenty three , eight thousand twenty four , eight thousand twenty five , eight thousand twenty six , eight thousand twenty seven , eight thousand twenty eight , eight thousand twenty nine , eight thousand thirty , eight thousand thirty one , eight thousand thirty two ,'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"o2aN-gcf22VK","colab_type":"code","outputId":"e326538f-69dc-40c0-d76a-defeb6398728","colab":{}},"source":["v.textify(x3[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'eight thousand thirty three , eight thousand thirty four , eight thousand thirty five , eight thousand thirty six , eight thousand thirty seven , eight thousand thirty eight , eight thousand thirty nine , eight thousand forty , eight thousand forty one , eight thousand forty two , eight thousand forty three , eight thousand forty four , eight thousand forty five , eight thousand forty six , eight'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"TlB-8uS4yKXI","colab_type":"text"},"source":["### batch 2"]},{"cell_type":"code","metadata":{"id":"_2WD990m22VN","colab_type":"code","outputId":"042102b1-36aa-4fd5-c04f-e8a35b3b369b","colab":{}},"source":["v.textify(x1[1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["', eight thousand forty six , eight thousand forty seven , eight thousand forty eight , eight thousand forty nine , eight thousand fifty , eight thousand fifty one , eight thousand fifty two , eight thousand fifty three , eight thousand fifty four , eight thousand fifty five , eight thousand fifty six , eight thousand fifty seven , eight thousand fifty eight , eight thousand fifty nine ,'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"wpBwoYK622VP","colab_type":"code","outputId":"a3adbbf5-ad4e-4ab1-9ae7-8c004bf0573d","colab":{}},"source":["v.textify(x2[1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'eight thousand sixty , eight thousand sixty one , eight thousand sixty two , eight thousand sixty three , eight thousand sixty four , eight thousand sixty five , eight thousand sixty six , eight thousand sixty seven , eight thousand sixty eight , eight thousand sixty nine , eight thousand seventy , eight thousand seventy one , eight thousand seventy two , eight thousand seventy three , eight thousand'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"GOWtwMnR22VT","colab_type":"code","outputId":"4b14acff-c320-44f7-efc0-0a2aa4a3cafe","colab":{}},"source":["v.textify(x3[1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'seventy four , eight thousand seventy five , eight thousand seventy six , eight thousand seventy seven , eight thousand seventy eight , eight thousand seventy nine , eight thousand eighty , eight thousand eighty one , eight thousand eighty two , eight thousand eighty three , eight thousand eighty four , eight thousand eighty five , eight thousand eighty six , eight thousand eighty seven , eight thousand eighty'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"1uCJ0EHw22VZ","colab_type":"code","outputId":"94386446-059f-4e02-dbe3-7da6c6be9442","colab":{}},"source":["v.textify(x3[-1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ninety , nine thousand nine hundred ninety one , nine thousand nine hundred ninety two , nine thousand nine hundred ninety three , nine thousand nine hundred ninety four , nine thousand nine hundred ninety five , nine thousand nine hundred ninety six , nine thousand nine hundred ninety seven , nine thousand nine hundred ninety eight , nine thousand nine hundred ninety nine xxbos eight thousand one , eight'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"markdown","metadata":{"id":"ycKtM1t37cnL","colab_type":"text"},"source":["NOTE:  Then we can go right back to the start, but look at batch index 1 which is batch number 2. Now we can continue. A slight skip from 8,040 to 8,046, that's because the last mini batch wasn't quite complete. What this means is that every mini batch joins up with a previous mini batch. So you can go straight from `x1[0]` to `x2[0]` - it continues 8,023, 8,024. If you took the same thing for `:,1`, you'll also see they join up. **So all the mini batches join up.**"]},{"cell_type":"code","metadata":{"id":"mfC46Lju22Vd","colab_type":"code","outputId":"ffae1e76-d2be-47c2-8522-63dacf5eb98d","colab":{}},"source":["data.show_batch(ds_type=DatasetType.Valid)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<table>  <col width='5%'>  <col width='95%'>  <tr>\n","    <th>idx</th>\n","    <th>text</th>\n","  </tr>\n","  <tr>\n","    <th>0</th>\n","    <th>thousand forty seven , eight thousand forty eight , eight thousand forty nine , eight thousand fifty , eight thousand fifty one , eight thousand fifty two , eight thousand fifty three , eight thousand fifty four , eight thousand fifty five , eight thousand fifty six , eight thousand fifty seven , eight thousand fifty eight , eight thousand fifty nine , eight thousand sixty , eight thousand sixty</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>eight , eight thousand eighty nine , eight thousand ninety , eight thousand ninety one , eight thousand ninety two , eight thousand ninety three , eight thousand ninety four , eight thousand ninety five , eight thousand ninety six , eight thousand ninety seven , eight thousand ninety eight , eight thousand ninety nine , eight thousand one hundred , eight thousand one hundred one , eight thousand one</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>thousand one hundred twenty four , eight thousand one hundred twenty five , eight thousand one hundred twenty six , eight thousand one hundred twenty seven , eight thousand one hundred twenty eight , eight thousand one hundred twenty nine , eight thousand one hundred thirty , eight thousand one hundred thirty one , eight thousand one hundred thirty two , eight thousand one hundred thirty three , eight thousand</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>three , eight thousand one hundred fifty four , eight thousand one hundred fifty five , eight thousand one hundred fifty six , eight thousand one hundred fifty seven , eight thousand one hundred fifty eight , eight thousand one hundred fifty nine , eight thousand one hundred sixty , eight thousand one hundred sixty one , eight thousand one hundred sixty two , eight thousand one hundred sixty three</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>thousand one hundred eighty three , eight thousand one hundred eighty four , eight thousand one hundred eighty five , eight thousand one hundred eighty six , eight thousand one hundred eighty seven , eight thousand one hundred eighty eight , eight thousand one hundred eighty nine , eight thousand one hundred ninety , eight thousand one hundred ninety one , eight thousand one hundred ninety two , eight thousand</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ZtEr13zn7ybO","colab_type":"text"},"source":["That's the data. We can do show_batch to see it."]},{"cell_type":"markdown","metadata":{"id":"prOSx5dU22Vh","colab_type":"text"},"source":["## Single fully connected model"]},{"cell_type":"code","metadata":{"id":"1Xp3dKfS22Vk","colab_type":"code","colab":{}},"source":["data = src.databunch(bs=bs, bptt=3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9IYMFVn0EBw","colab_type":"text"},"source":["NOTE: here we have a length sequence of 3 not 70 as before."]},{"cell_type":"code","metadata":{"id":"kDQgSkRI22Vq","colab_type":"code","outputId":"ce2005b4-3493-438d-bf02-b2ae93f94e20","colab":{}},"source":["x,y = data.one_batch()\n","x.shape,y.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 3]), torch.Size([64, 3]))"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"l13LrxnD22Vt","colab_type":"code","outputId":"cbec0db3-636d-4364-9d52-01ac08372d75","colab":{}},"source":["nv = len(v.itos); nv"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"DHwoTHFi22V8","colab_type":"code","colab":{}},"source":["nh=64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA1blRol22V-","colab_type":"code","colab":{}},"source":["def loss4(input,target): return F.cross_entropy(input, target[:,-1])\n","def acc4 (input,target): return accuracy(input, target[:,-1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"niRFtjnX970r","colab_type":"text"},"source":["NOTE:  \n","`F.cross_entroy(input, target(:-1)] it will compare the result of our model (`input` to the last word in the sequence `target(:-1)`.\n","`\n"]},{"cell_type":"markdown","metadata":{"id":"hqmO4Ha979N5","colab_type":"text"},"source":["Here is our model which is doing what we saw in the diagram:"]},{"cell_type":"code","metadata":{"id":"1fzvz6y522WA","colab_type":"code","colab":{}},"source":["# model definition as per diagram\n","class Model0(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.i_h = nn.Embedding(nv,nh)  # green arrow\n","        self.h_h = nn.Linear(nh,nh)     # brown arrow\n","        self.h_o = nn.Linear(nh,nv)     # blue arrow\n","        self.bn = nn.BatchNorm1d(nh)\n","        \n","    def forward(self, x):\n","        h = self.bn(F.relu(self.i_h(x[:,0])))\n","        if x.shape[1]>1:\n","            h = h + self.i_h(x[:,1])\n","            h = self.bn(F.relu(self.h_h(h)))\n","        if x.shape[1]>2:\n","            h = h + self.i_h(x[:,2])\n","            h = self.bn(F.relu(self.h_h(h)))\n","        return self.h_o(h)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0Yt0qjy8Gwg","colab_type":"text"},"source":["NOTE:  It content contains 1 embedding (i.e. the green arrow), one hidden to hidden - brown arrow layer, and one hidden to output. So each colored arrow has a single matrix. Then in the forward pass, we take our first input `x[0]` and put it through input to hidden (the green arrow), create our first set of activations which we call `h`. Assuming that there is a second word, because sometimes we might be at the end of a batch where there isn't a second word. Assume there is a second word then we would add to h the result of `x[1]` put through the green arrow (that's `i_h`). Then we would say, okay our new `h` is the result of those two added together, put through our hidden to hidden (orange arrow), and then ReLU then batch norm. Then for the second word, do exactly the same thing. Then finally blue arrow - put it through `h_o`.\n","\n","So that's how we convert our diagram to code. Nothing new here at all. We can chuck that in a learner, and we can train it - 46%."]},{"cell_type":"code","metadata":{"id":"eb_2HTaX22WG","colab_type":"code","colab":{}},"source":["learn = Learner(data, Model0(), loss_func=loss4, metrics=acc4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAk5lC6p22WJ","colab_type":"code","outputId":"33dac9b2-bc89-4d46-ec38-4d91956884b0","colab":{}},"source":["learn.fit_one_cycle(6, 1e-4)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 00:07 <p><table style='width:300px; margin-bottom:10px'>\n","  <tr>\n","    <th>epoch</th>\n","    <th>train_loss</th>\n","    <th>valid_loss</th>\n","    <th>acc4</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>3.596286</th>\n","    <th>3.588869</th>\n","    <th>0.046645</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>3.086100</th>\n","    <th>3.205763</th>\n","    <th>0.274816</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>2.494411</th>\n","    <th>2.749365</th>\n","    <th>0.392004</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>2.144753</th>\n","    <th>2.463537</th>\n","    <th>0.415671</th>\n","  </tr>\n","  <tr>\n","    <th>5</th>\n","    <th>2.010915</th>\n","    <th>2.352887</th>\n","    <th>0.409237</th>\n","  </tr>\n","  <tr>\n","    <th>6</th>\n","    <th>1.983992</th>\n","    <th>2.336967</th>\n","    <th>0.408778</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JoA5D0Jg22WN","colab_type":"text"},"source":["## Same thing with a loop\n","\n","[video timing](https://www.youtube.com/watch?v=nWpdkZE2_cc&feature=youtu.be&t=6648)\n","\n","Let's take this code and recognize it's pretty awful. There's a lot of duplicate code, and as coders, when we see duplicate code, what do we do? We refactor. So we should refactor this into a loop.\n","\n"]},{"cell_type":"code","metadata":{"id":"SSCcTtiX22WO","colab_type":"code","colab":{}},"source":["class Model1(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.i_h = nn.Embedding(nv,nh)  # green arrow\n","        self.h_h = nn.Linear(nh,nh)     # brown arrow\n","        self.h_o = nn.Linear(nh,nv)     # blue arrow\n","        self.bn = nn.BatchNorm1d(nh)\n","        \n","    def forward(self, x):\n","        h = torch.zeros(x.shape[0], nh).to(device=x.device)\n","        for i in range(x.shape[1]):\n","            h = h + self.i_h(x[:,i])\n","            h = self.bn(F.relu(self.h_h(h)))\n","        return self.h_o(h)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgHNuTSM8zUA","colab_type":"text"},"source":["NOTE:  Here we are. We've refactored it into a loop. So now we're going for each `xi` in `x`, and doing it in the loop. Guess what? **That's an RNN**. An RNN is just a refactoring. It's not anything new. This is now an RNN. And let's refactor our diagram:\n","\n","This is the same diagram, but I've just replaced it with my loop. It does the same thing, so here it is. It's got exactly the same `__init__`, literally exactly the same, just popped a loop here. Before I start, I just have to make sure that I've got a bunch of zeros to add to. And of course, I get exactly the same result when I train it.\n","\n","Now, this code will work for any arbitrary lengh sequence. So for the next section of this notebook, we will start with a bptt = 20."]},{"cell_type":"code","metadata":{"id":"8BxJNyhS22WS","colab_type":"code","colab":{}},"source":["learn = Learner(data, Model1(), loss_func=loss4, metrics=acc4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"trVZ28vh22WV","colab_type":"code","outputId":"fda5566d-4d97-48d3-c62a-b3cc032fd2a4","colab":{}},"source":["learn.fit_one_cycle(6, 1e-4)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 00:07 <p><table style='width:300px; margin-bottom:10px'>\n","  <tr>\n","    <th>epoch</th>\n","    <th>train_loss</th>\n","    <th>valid_loss</th>\n","    <th>acc4</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>3.493525</th>\n","    <th>3.420231</th>\n","    <th>0.156250</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>2.987600</th>\n","    <th>2.937893</th>\n","    <th>0.376149</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>2.440199</th>\n","    <th>2.477995</th>\n","    <th>0.388787</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>2.132837</th>\n","    <th>2.256569</th>\n","    <th>0.391774</th>\n","  </tr>\n","  <tr>\n","    <th>5</th>\n","    <th>2.011305</th>\n","    <th>2.181337</th>\n","    <th>0.392923</th>\n","  </tr>\n","  <tr>\n","    <th>6</th>\n","    <th>1.985913</th>\n","    <th>2.170874</th>\n","    <th>0.393153</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"sHj5Yv4x9YMq","colab_type":"text"},"source":["NOTE:  One nice thing about the loop though, is now this will work even if I'm not predicting the fourth word from the previous three, but the ninth word from the previous eight. It'll work for any arbitrarily length long sequence which is nice.\n","\n","So let's up the `bptt` to 20 since we can now. And let's now say, okay, instead of just predicting the `n`th word from the previous `n-1`, let's try to predict the second word from the first, the third from the second, and the fourth from the third, and so forth. Look at our loss function."]},{"cell_type":"markdown","metadata":{"id":"MC4Co6lM22Wa","colab_type":"text"},"source":["## Multi fully connected model"]},{"cell_type":"markdown","metadata":{"id":"ZnZZ_ZL7-fJ7","colab_type":"text"},"source":["In other words, after every loop, predict, loop, predict, loop, predict.\n","\n"]},{"cell_type":"code","metadata":{"id":"VZfGt-Qr22Wb","colab_type":"code","colab":{}},"source":["data = src.databunch(bs=bs, bptt=20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z00v15fL2ubz","colab_type":"text"},"source":["`bptt = 20` sequence length is now 20."]},{"cell_type":"code","metadata":{"id":"0uVry8DI22Wf","colab_type":"code","outputId":"7710c3ae-28cf-4d47-b6de-261ef9dbb037","colab":{}},"source":["x,y = data.one_batch()\n","x.shape,y.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 20]), torch.Size([64, 20]))"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"J04vm1Qr22Wk","colab_type":"code","colab":{}},"source":["class Model2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.i_h = nn.Embedding(nv,nh)\n","        self.h_h = nn.Linear(nh,nh)\n","        self.h_o = nn.Linear(nh,nv)\n","        self.bn = nn.BatchNorm1d(nh)\n","        \n","    def forward(self, x):\n","        h = torch.zeros(x.shape[0], nh).to(device=x.device)\n","        res = []\n","        for i in range(x.shape[1]):\n","            h = h + self.i_h(x[:,i])\n","            h = F.relu(self.h_h(h))\n","            res.append(self.h_o(self.bn(h)))\n","        return torch.stack(res, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obIJvYkK-viR","colab_type":"text"},"source":["NOTE:  Previously we were comparing the result of our model to just the last word of the sequence. It is very wasteful, because there's a lot of words in the sequence. So let's compare every word in x to every word and y. To do that, we need to change the diagram so it's not just one triangle at the end of the loop, but the triangle is inside the loop: \n","\n","Here's this code. It's the same as the previous code, but now I've created **an array**, and every time I go through the loop, I append `h_o(h)` to the array.  So, now, for `n` inputs, I create `n` outputs. **So I'm predicting after every word**."]},{"cell_type":"code","metadata":{"id":"BABw-PaY22Wn","colab_type":"code","colab":{}},"source":["learn = Learner(data, Model2(), metrics=accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTYH9-Rb22Wp","colab_type":"code","outputId":"28a8ea11-29f1-4ea9-eb34-6c8887e65f30","colab":{}},"source":["learn.fit_one_cycle(10, 1e-4, pct_start=0.1)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 00:06 <p><table style='width:300px; margin-bottom:10px'>\n","  <tr>\n","    <th>epoch</th>\n","    <th>train_loss</th>\n","    <th>valid_loss</th>\n","    <th>accuracy</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>3.639285</th>\n","    <th>3.709278</th>\n","    <th>0.058949</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>3.551151</th>\n","    <th>3.565677</th>\n","    <th>0.151776</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>3.439908</th>\n","    <th>3.431850</th>\n","    <th>0.207741</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>3.323083</th>\n","    <th>3.314237</th>\n","    <th>0.283949</th>\n","  </tr>\n","  <tr>\n","    <th>5</th>\n","    <th>3.213422</th>\n","    <th>3.219906</th>\n","    <th>0.321662</th>\n","  </tr>\n","  <tr>\n","    <th>6</th>\n","    <th>3.119673</th>\n","    <th>3.151162</th>\n","    <th>0.336790</th>\n","  </tr>\n","  <tr>\n","    <th>7</th>\n","    <th>3.046645</th>\n","    <th>3.106630</th>\n","    <th>0.341690</th>\n","  </tr>\n","  <tr>\n","    <th>8</th>\n","    <th>2.995379</th>\n","    <th>3.082552</th>\n","    <th>0.346662</th>\n","  </tr>\n","  <tr>\n","    <th>9</th>\n","    <th>2.963800</th>\n","    <th>3.073327</th>\n","    <th>0.349645</th>\n","  </tr>\n","  <tr>\n","    <th>10</th>\n","    <th>2.947312</th>\n","    <th>3.071951</th>\n","    <th>0.349787</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"c_SX5iSO_B1U","colab_type":"text"},"source":["NOTE:  Previously I had 39.%, now I have 34.9%. **Why is it worse? It's worse because now when I'm trying to predict the second word, I only have one word of state to use.** When I'm looking at the third word, I only have two words of state to use. So it's a much harder problem for it to solve. The key problem is here:\n","\n","`h = torch.zeros(x.shape[0], nh).to(device=x.device)`\n","\n","```\n","def forward(self, x):\n","       ** h = torch.zeros(x.shape[0], nh).to(device=x.device)**\n","        for i in range(x.shape[1]):\n","            h = h + self.i_h(x[:,i])\n","            h = self.bn(F.relu(self.h_h(h)))\n","        return self.h_o(h)\n","```\n","I am restarting the state after every bptt sequence.  So let's take away this line from the forward method and put it in the __init__ section. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DfifDShC22Ws","colab_type":"text"},"source":["## Maintain state"]},{"cell_type":"code","metadata":{"id":"0WdJ0B5W22Wt","colab_type":"code","colab":{}},"source":["class Model3(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.i_h = nn.Embedding(nv,nh)\n","        self.h_h = nn.Linear(nh,nh)\n","        self.h_o = nn.Linear(nh,nv)\n","        self.bn = nn.BatchNorm1d(nh)\n","        self.h = torch.zeros(bs, nh).cuda()\n","        \n","    def forward(self, x):\n","        res = []\n","        h = self.h\n","        for i in range(x.shape[1]):\n","            h = h + self.i_h(x[:,i])\n","            h = F.relu(self.h_h(h))\n","            res.append(self.bn(h))\n","        self.h = h.detach()\n","        res = torch.stack(res, dim=1)\n","        res = self.h_o(res)\n","        return res"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJKLRUOS_dJB","colab_type":"text"},"source":["NOTE:  There it is. So it's now `self.h`. So this is now exactly the same code, but at the end, let's put the new h back into `self.h`. It's now doing the same thing, but it's not throwing away that state."]},{"cell_type":"markdown","metadata":{"id":"7esVy5ni_PEY","colab_type":"text"},"source":["NOTE:  I go `h = torch.zeros`. I reset my state to zero every time I start another BPTT sequence. Let's not do that. Let's keep `h`. And we can, because remember, each batch connects to the previous batch. It's not shuffled like happens in image classification. So let's take this exact model and replicate it again, but let's move the creation of h into the constructor."]},{"cell_type":"code","metadata":{"id":"TA9yzGxM22Wu","colab_type":"code","colab":{}},"source":["learn = Learner(data, Model3(), metrics=accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"66DFnL5y22Ww","colab_type":"code","outputId":"06ea06cd-f545-450f-dab1-f7443489e576","colab":{}},"source":["learn.fit_one_cycle(20, 3e-3)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 00:11 <p><table style='width:300px; margin-bottom:10px'>\n","  <tr>\n","    <th>epoch</th>\n","    <th>train_loss</th>\n","    <th>valid_loss</th>\n","    <th>accuracy</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>3.598183</th>\n","    <th>3.556362</th>\n","    <th>0.050710</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>3.274616</th>\n","    <th>2.975699</th>\n","    <th>0.401634</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>2.624206</th>\n","    <th>2.036894</th>\n","    <th>0.467330</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>2.022702</th>\n","    <th>1.956439</th>\n","    <th>0.316193</th>\n","  </tr>\n","  <tr>\n","    <th>5</th>\n","    <th>1.681813</th>\n","    <th>1.934952</th>\n","    <th>0.336861</th>\n","  </tr>\n","  <tr>\n","    <th>6</th>\n","    <th>1.453007</th>\n","    <th>1.948201</th>\n","    <th>0.351349</th>\n","  </tr>\n","  <tr>\n","    <th>7</th>\n","    <th>1.276971</th>\n","    <th>2.005776</th>\n","    <th>0.368679</th>\n","  </tr>\n","  <tr>\n","    <th>8</th>\n","    <th>1.138499</th>\n","    <th>2.081261</th>\n","    <th>0.360156</th>\n","  </tr>\n","  <tr>\n","    <th>9</th>\n","    <th>1.029217</th>\n","    <th>2.145853</th>\n","    <th>0.360795</th>\n","  </tr>\n","  <tr>\n","    <th>10</th>\n","    <th>0.939949</th>\n","    <th>2.215388</th>\n","    <th>0.372230</th>\n","  </tr>\n","  <tr>\n","    <th>11</th>\n","    <th>0.865441</th>\n","    <th>2.240438</th>\n","    <th>0.401491</th>\n","  </tr>\n","  <tr>\n","    <th>12</th>\n","    <th>0.805310</th>\n","    <th>2.195846</th>\n","    <th>0.409375</th>\n","  </tr>\n","  <tr>\n","    <th>13</th>\n","    <th>0.755035</th>\n","    <th>2.324373</th>\n","    <th>0.422727</th>\n","  </tr>\n","  <tr>\n","    <th>14</th>\n","    <th>0.713073</th>\n","    <th>2.305542</th>\n","    <th>0.449716</th>\n","  </tr>\n","  <tr>\n","    <th>15</th>\n","    <th>0.677393</th>\n","    <th>2.350155</th>\n","    <th>0.446449</th>\n","  </tr>\n","  <tr>\n","    <th>16</th>\n","    <th>0.645841</th>\n","    <th>2.418738</th>\n","    <th>0.446591</th>\n","  </tr>\n","  <tr>\n","    <th>17</th>\n","    <th>0.621809</th>\n","    <th>2.456903</th>\n","    <th>0.446165</th>\n","  </tr>\n","  <tr>\n","    <th>18</th>\n","    <th>0.605300</th>\n","    <th>2.541699</th>\n","    <th>0.443040</th>\n","  </tr>\n","  <tr>\n","    <th>19</th>\n","    <th>0.594099</th>\n","    <th>2.539824</th>\n","    <th>0.443040</th>\n","  </tr>\n","  <tr>\n","    <th>20</th>\n","    <th>0.587563</th>\n","    <th>2.551423</th>\n","    <th>0.442827</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"14abdqCe_onI","colab_type":"text"},"source":["NOTE:  Therefore, now we actually get above the original. We get all the way up to 44% accuracy. So this is what a real RNN looks like. You always want to keep that state. But just keep remembering, there's nothing different about an RNN, and it's a totally normal fully connected neural net. It's just that you've got a loop you refactored."]},{"cell_type":"markdown","metadata":{"id":"9Jiaj_SP22Wy","colab_type":"text"},"source":["## nn.RNN"]},{"cell_type":"markdown","metadata":{"id":"PuTswmF9_6x-","colab_type":"text"},"source":["NOTE:  What you could do though is at the end of your every loop, you could not just spit out an output, but you could spit it out into another RNN. So you have an RNN going into an RNN. That's nice because we now got more layers of computation, you would expect that to work better.\n","\n","To get there, let's do some more refactoring. Let's take this code (`Model3`) and replace it with the equivalent built in PyTorch code which is you just say that:"]},{"cell_type":"code","metadata":{"id":"b8-Swcby22Wz","colab_type":"code","colab":{}},"source":["class Model4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.i_h = nn.Embedding(nv,nh)\n","        self.rnn = nn.RNN(nh,nh, batch_first=True)\n","        self.h_o = nn.Linear(nh,nv)\n","        self.bn = BatchNorm1dFlat(nh)\n","        self.h = torch.zeros(1, bs, nh).cuda()\n","        \n","    def forward(self, x):\n","        res,h = self.rnn(self.i_h(x), self.h)\n","        self.h = h.detach()\n","        return self.h_o(self.bn(res))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HhFM21ehAE_f","colab_type":"text"},"source":["NOTE:  So `nn.RNN` basically says do the loop for me. We've still got the same embedding, we've still got the same output, still got the same batch norm, we still got the same initialization of `h`, but we just got rid of the loop. One of the nice things about RNN is that you can now say how many layers you want. This is the same accuracy of course:\n","\n"]},{"cell_type":"code","metadata":{"id":"pR3eDks822W3","colab_type":"code","colab":{}},"source":["learn = Learner(data, Model4(), metrics=accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDm2HofL22W7","colab_type":"code","outputId":"8fa6be2c-d246-4641-d8aa-4f175ca37d2c","colab":{}},"source":["learn.fit_one_cycle(20, 3e-3)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 00:04 <p><table style='width:300px; margin-bottom:10px'>\n","  <tr>\n","    <th>epoch</th>\n","    <th>train_loss</th>\n","    <th>valid_loss</th>\n","    <th>accuracy</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>3.451432</th>\n","    <th>3.268344</th>\n","    <th>0.224148</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>2.974938</th>\n","    <th>2.456569</th>\n","    <th>0.466051</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>2.316732</th>\n","    <th>1.946969</th>\n","    <th>0.465625</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>1.866151</th>\n","    <th>1.991952</th>\n","    <th>0.314702</th>\n","  </tr>\n","  <tr>\n","    <th>5</th>\n","    <th>1.618516</th>\n","    <th>1.802403</th>\n","    <th>0.437216</th>\n","  </tr>\n","  <tr>\n","    <th>6</th>\n","    <th>1.411517</th>\n","    <th>1.731107</th>\n","    <th>0.436293</th>\n","  </tr>\n","  <tr>\n","    <th>7</th>\n","    <th>1.171916</th>\n","    <th>1.655979</th>\n","    <th>0.504048</th>\n","  </tr>\n","  <tr>\n","    <th>8</th>\n","    <th>0.965887</th>\n","    <th>1.579963</th>\n","    <th>0.522088</th>\n","  </tr>\n","  <tr>\n","    <th>9</th>\n","    <th>0.797046</th>\n","    <th>1.479819</th>\n","    <th>0.565057</th>\n","  </tr>\n","  <tr>\n","    <th>10</th>\n","    <th>0.659378</th>\n","    <th>1.487831</th>\n","    <th>0.579048</th>\n","  </tr>\n","  <tr>\n","    <th>11</th>\n","    <th>0.553282</th>\n","    <th>1.441922</th>\n","    <th>0.597798</th>\n","  </tr>\n","  <tr>\n","    <th>12</th>\n","    <th>0.475167</th>\n","    <th>1.498148</th>\n","    <th>0.600781</th>\n","  </tr>\n","  <tr>\n","    <th>13</th>\n","    <th>0.416131</th>\n","    <th>1.546984</th>\n","    <th>0.606463</th>\n","  </tr>\n","  <tr>\n","    <th>14</th>\n","    <th>0.372395</th>\n","    <th>1.594261</th>\n","    <th>0.607386</th>\n","  </tr>\n","  <tr>\n","    <th>15</th>\n","    <th>0.337093</th>\n","    <th>1.578321</th>\n","    <th>0.613352</th>\n","  </tr>\n","  <tr>\n","    <th>16</th>\n","    <th>0.311385</th>\n","    <th>1.580973</th>\n","    <th>0.623366</th>\n","  </tr>\n","  <tr>\n","    <th>17</th>\n","    <th>0.292869</th>\n","    <th>1.625745</th>\n","    <th>0.618253</th>\n","  </tr>\n","  <tr>\n","    <th>18</th>\n","    <th>0.279486</th>\n","    <th>1.623960</th>\n","    <th>0.626065</th>\n","  </tr>\n","  <tr>\n","    <th>19</th>\n","    <th>0.270054</th>\n","    <th>1.682090</th>\n","    <th>0.611719</th>\n","  </tr>\n","  <tr>\n","    <th>20</th>\n","    <th>0.263857</th>\n","    <th>1.675676</th>\n","    <th>0.614702</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"-JF0FeSkAUiJ","colab_type":"text"},"source":["So here, I'm going to do it with two layers:"]},{"cell_type":"markdown","metadata":{"id":"HXexHsWi22W_","colab_type":"text"},"source":["## 2-layer GRU\n","\n","But here's the thing. When you think about this: (see picture)\n","\n","It keeps on going, and we've got a BPTT of 20, so there's 20 layers of this. And we know from that visualizing the loss landscapes paper, that deep networks have awful bumpy loss surfaces. So when you start creating long timescales and multiple layers, these things get impossible to train. There's a few tricks you can do. One thing is you can add skip connections, of course. But what people normally do is, instead of just adding these together(green and orange arrows), they actually use a little mini neural net to decide how much of the green arrow to keep and how much of the orange arrow to keep. When you do that, you get something that's either called GRU or LSTM depending on the details of that little neural net. And we'll learn about the details of those neural nets in part 2. They really don't matter though, frankly.\n","\n","So we can now say let's create a GRU instead. It's just like what we had before, but it'll handle longer sequences in deeper networks. Let's use two layers."]},{"cell_type":"code","metadata":{"id":"EnsOkinK22XA","colab_type":"code","colab":{}},"source":["class Model5(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.i_h = nn.Embedding(nv,nh)\n","        self.rnn = nn.GRU(nh, nh, 2, batch_first=True)\n","        self.h_o = nn.Linear(nh,nv)\n","        self.bn = BatchNorm1dFlat(nh)\n","        self.h = torch.zeros(2, bs, nh).cuda()\n","        \n","    def forward(self, x):\n","        res,h = self.rnn(self.i_h(x), self.h)\n","        self.h = h.detach()\n","        return self.h_o(self.bn(res))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4Qe9i4f22XE","colab_type":"code","colab":{}},"source":["learn = Learner(data, Model5(), metrics=accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFUYPtHj22XH","colab_type":"code","outputId":"9e6729ad-b222-4faf-c33a-95426e08545d","colab":{}},"source":["learn.fit_one_cycle(10, 1e-2)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 00:02 <p><table style='width:300px; margin-bottom:10px'>\n","  <tr>\n","    <th>epoch</th>\n","    <th>train_loss</th>\n","    <th>valid_loss</th>\n","    <th>accuracy</th>\n","  </tr>\n","  <tr>\n","    <th>1</th>\n","    <th>2.864854</th>\n","    <th>2.314943</th>\n","    <th>0.454545</th>\n","  </tr>\n","  <tr>\n","    <th>2</th>\n","    <th>1.798988</th>\n","    <th>1.357116</th>\n","    <th>0.629688</th>\n","  </tr>\n","  <tr>\n","    <th>3</th>\n","    <th>0.932729</th>\n","    <th>1.307463</th>\n","    <th>0.796733</th>\n","  </tr>\n","  <tr>\n","    <th>4</th>\n","    <th>0.451969</th>\n","    <th>1.329699</th>\n","    <th>0.788636</th>\n","  </tr>\n","  <tr>\n","    <th>5</th>\n","    <th>0.225787</th>\n","    <th>1.293570</th>\n","    <th>0.800142</th>\n","  </tr>\n","  <tr>\n","    <th>6</th>\n","    <th>0.118085</th>\n","    <th>1.265926</th>\n","    <th>0.803338</th>\n","  </tr>\n","  <tr>\n","    <th>7</th>\n","    <th>0.065306</th>\n","    <th>1.207096</th>\n","    <th>0.806960</th>\n","  </tr>\n","  <tr>\n","    <th>8</th>\n","    <th>0.038098</th>\n","    <th>1.205361</th>\n","    <th>0.813920</th>\n","  </tr>\n","  <tr>\n","    <th>9</th>\n","    <th>0.024069</th>\n","    <th>1.239411</th>\n","    <th>0.807813</th>\n","  </tr>\n","  <tr>\n","    <th>10</th>\n","    <th>0.017078</th>\n","    <th>1.253409</th>\n","    <th>0.807102</th>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"N1GfaBcmAnet","colab_type":"text"},"source":["NOTE:  And we're up to 75%. That's RNNs and the main reason I wanted to show it to you was to remove the last remaining piece of magic, and this is one of the least magical things we have in deep learning. It's just a refactored fully connected network. So don't let RNNs ever put you off. With this approach where you basically have a sequence of n inputs and a sequence of n outputs we've been using for language modeling, you can use that for other tasks.\n","\n","For example, the sequence of outputs could be for every word there could be something saying is there something that is sensitive and I want to anonymize or not. So it says private data or not. Or it could be a part of speech tag for that word, or it could be something saying how should that word be formatted, or whatever. These are called sequence labeling tasks and so you can use this same approach for pretty much any sequence labeling task. Or you can do what I did in the earlier lesson which is once you finish building your language model, you can throw away the h_o bit, and instead pop there a standard classification head, and then you can now do NLP classification which as you saw earlier will give you a state of the art results even on long documents. So this is a super valuable technique, and not remotely magical."]},{"cell_type":"markdown","metadata":{"id":"qv-o3cWW22XJ","colab_type":"text"},"source":["# Fast.AI Wrap up\n","\n","## What we did cover in Part 1\n","- Affine Functions & Non-Linearities\n","- Batch Norm\n","- Image classification & Regression\n","- Segmentation, U-Net, GANs\n","- Parameters and Activations\n","- Dropout\n","- Embeddings\n","- SGD, Momemtum, Adam\n","- Convolutions\n","- Res/Dense Block\n","- Language Modesl, NLP classification\n","- Weight Decay\n","- Collaborative Filtering\n","- Random init & Transfer Learning\n","- Data augmentation\n","- Continuous and Categorical variables\n","\n","\n","## Advices\n","That's it. That's deep learning, or at least the practical pieces from my point of view. Having watched this one time, you won't get it all. And I don't recommend that you do watch this so slowly that you get it all the first time, but you go back and look at it again, take your time, and there'll be bits that you go like \"oh, now I see what he's saying\" and then you'll be able to implement things you couldn't before or you'll be able to dig in more than before. So definitely go back and do it again. And as you do, write code, not just for yourself but put it on github. It doesn't matter if you think it's great code or not. The fact that you're writing code and sharing it is impressive and the feedback you'll get if you tell people on the forum \"hey, I wrote this code. It's not great but it's my first effort. Anything you see jump out at you?\" People will say like \"oh, that bit was done well. But you know, for this bit, you could have used this library and saved you sometime.\" You'll learn a lot by interacting with your peers.\n","\n","As you've noticed, I've started introducing more and more papers. Part 2 will be a lot of papers, so it's a good time to start reading some of the papers that have been introduced in this section. **All the bits that say like derivation and theorems and lemmas, you can skip them. I do. They add almost nothing to your understanding of your practical deep learning**. **But the bits that say why are we solving this problem, and what are the results, and so forth, are really interesting. Then try and write English prose.** Not English prose that you want to be read by Geoffrey Hinton and Yann LeCun, but English prose you want to be read by you as of six months ago. Because there's a lot more people in the audience of you as of six months ago than there is of Geoffrey Hinton and Yann LeCun. That's the person you best understand. You know what they need.\n","\n","Go and get help, and help others. Tell us about your success stories. But perhaps the most important one is get together with others. People's learning works much better if you've got that social experience. So start a book club, get involved in meetups, create study groups, and build things. Again, it doesn't have to be amazing. Just build something that you think the world would be a little bit better if that existed. Or you think it would be kind of slightly delightful to your two-year-old to see that thing. Or you just want to show it to your brother the next time they come around to see what you're doing, whatever. Just finish something. Finish something and then try make it a bit better.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v4VHIbVrBuXg","colab_type":"text"},"source":["## Fast.AI Part 2\n","\n","What is coming up?\n","\n","- Deep dive into Fast.ai codebase\n","- Development and Research Process\n","- Reading Academic Papers\n","- Translating Math to Code\n","- Attentional Models\n","- Speech Recognition\n","- Translation\n","- Multi-Modal Models\n","- CycleGAN\n","- Object Detection\n","- Large and Distributed Training\n","- and more ...\n"]},{"cell_type":"code","metadata":{"id":"6ElNxJaO22XK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}